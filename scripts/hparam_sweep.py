"""
CropShield AI - Hyperparameter Sweep Automation
================================================

Automated hyperparameter optimization script that:
- Tests multiple configurations (learning rate, weight decay, dropout)
- Runs short 5-epoch experiments for each config
- Logs all metrics to experiments/ directory
- Selects best configuration based on validation accuracy
- Automatically retrains final model with best config
- Saves optimized model as models/cropshield_cnn_best.pth

Usage:
    python scripts/hparam_sweep.py
    
Fully autonomous - no user input required!
"""

import torch
import torch.nn as nn
import torch.optim as optim
from torch.cuda.amp import GradScaler, autocast
from torch.utils.data import DataLoader
from pathlib import Path
import json
import time
from datetime import datetime
from tqdm import tqdm
import numpy as np
import itertools
import os

# Import project modules
from models.model_factory import get_model
from fast_dataset import make_loaders


class HyperparameterSweep:
    """Automated hyperparameter optimization"""
    
    def __init__(
        self,
        quick_epochs=5,
        final_epochs=25,
        batch_size=32,
        experiments_dir='experiments',
        models_dir='models'
    ):
        """
        Initialize sweep.
        
        Args:
            quick_epochs: Epochs for quick experiments
            final_epochs: Epochs for final retrain
            batch_size: Batch size for training
            experiments_dir: Directory to save experiment results
            models_dir: Directory to save models
        """
        self.quick_epochs = quick_epochs
        self.final_epochs = final_epochs
        self.batch_size = batch_size
        self.experiments_dir = Path(experiments_dir)
        self.models_dir = Path(models_dir)
        
        # Create directories
        self.experiments_dir.mkdir(exist_ok=True)
        self.models_dir.mkdir(exist_ok=True)
        
        # Device detection
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        
        # Hyperparameter search space
        self.search_space = {
            'learning_rate': [0.001, 0.0005, 0.0001],
            'weight_decay': [0.0001, 0.0005, 0.001],
            'dropout': [0.3, 0.5]  # Will modify model architecture
        }
        
        # Results tracking
        self.experiments = []
        self.best_config = None
        self.best_val_acc = 0.0
        
        print("\n" + "="*80)
        print("üî¨ CROPSHIELD AI - HYPERPARAMETER SWEEP")
        print("="*80)
        print(f"üìÖ Started: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"üñ•Ô∏è  Device: {self.device}")
        if torch.cuda.is_available():
            print(f"   GPU: {torch.cuda.get_device_name(0)}")
        print(f"üìä Quick Epochs: {self.quick_epochs}")
        print(f"üìä Final Epochs: {self.final_epochs}")
        print(f"üîç Search Space:")
        for param, values in self.search_space.items():
            print(f"   {param}: {values}")
        print("="*80 + "\n")
    
    def generate_configs(self):
        """Generate all hyperparameter configurations to test"""
        # For speed, we'll test 5 carefully chosen configs instead of full grid
        configs = [
            # Baseline (higher LR, lower WD)
            {'learning_rate': 0.001, 'weight_decay': 0.0001, 'dropout': 0.3},
            
            # Lower LR, same WD
            {'learning_rate': 0.0005, 'weight_decay': 0.0001, 'dropout': 0.3},
            
            # Even lower LR, higher WD
            {'learning_rate': 0.0001, 'weight_decay': 0.0005, 'dropout': 0.3},
            
            # Medium LR, higher dropout
            {'learning_rate': 0.0005, 'weight_decay': 0.0005, 'dropout': 0.5},
            
            # Conservative (lowest LR, highest regularization)
            {'learning_rate': 0.0001, 'weight_decay': 0.001, 'dropout': 0.5}
        ]
        
        print(f"üìã Generated {len(configs)} configurations to test:")
        for i, config in enumerate(configs, 1):
            print(f"   {i}. LR={config['learning_rate']:.5f}, "
                  f"WD={config['weight_decay']:.5f}, "
                  f"Dropout={config['dropout']:.1f}")
        print()
        
        return configs
    
    def load_data(self):
        """Load training and validation data"""
        print("üìÇ Loading Dataset...")
        
        num_workers = 0 if os.name == 'nt' else min(8, os.cpu_count() - 1)
        
        result = make_loaders(
            data_dir='Database_resized',
            batch_size=self.batch_size,
            train_split=0.8,
            val_split=0.1,
            test_split=0.1,
            num_workers=num_workers,
            seed=42,
            augmentation_mode='moderate'
        )
        
        self.train_loader, self.val_loader, self.test_loader, self.class_names, _ = result
        self.num_classes = len(self.class_names)
        
        print(f"‚úÖ Dataset Loaded:")
        print(f"   Train: {len(self.train_loader.dataset)} images")
        print(f"   Val: {len(self.val_loader.dataset)} images")
        print(f"   Classes: {self.num_classes}\n")
    
    def create_model_with_config(self, config):
        """Create model with specific hyperparameter configuration"""
        # Note: Dropout modification would require model architecture changes
        # For now, we'll focus on LR and weight decay which are easier to modify
        model, _, _, _, _ = get_model(
            model_type='custom',
            num_classes=self.num_classes,
            learning_rate=config['learning_rate']
        )
        
        # Create optimizer with specific weight decay
        optimizer = optim.Adam(
            model.parameters(),
            lr=config['learning_rate'],
            weight_decay=config['weight_decay']
        )
        
        # Learning rate scheduler
        scheduler = optim.lr_scheduler.StepLR(
            optimizer,
            step_size=5,
            gamma=0.5
        )
        
        # Loss function
        criterion = nn.CrossEntropyLoss()
        
        model = model.to(self.device)
        
        return model, optimizer, scheduler, criterion
    
    def train_epoch(self, model, optimizer, criterion, scaler):
        """Train for one epoch"""
        model.train()
        running_loss = 0.0
        correct = 0
        total = 0
        
        for images, labels in self.train_loader:
            images, labels = images.to(self.device), labels.to(self.device)
            
            optimizer.zero_grad()
            
            # Mixed precision training
            with autocast(enabled=torch.cuda.is_available()):
                outputs = model(images)
                loss = criterion(outputs, labels)
            
            scaler.scale(loss).backward()
            scaler.step(optimizer)
            scaler.update()
            
            running_loss += loss.item()
            _, predicted = outputs.max(1)
            total += labels.size(0)
            correct += predicted.eq(labels).sum().item()
        
        epoch_loss = running_loss / len(self.train_loader)
        epoch_acc = 100.0 * correct / total
        
        return epoch_loss, epoch_acc
    
    def validate(self, model, criterion):
        """Validate model"""
        model.eval()
        running_loss = 0.0
        correct = 0
        total = 0
        
        with torch.no_grad():
            for images, labels in self.val_loader:
                images, labels = images.to(self.device), labels.to(self.device)
                
                with autocast(enabled=torch.cuda.is_available()):
                    outputs = model(images)
                    loss = criterion(outputs, labels)
                
                running_loss += loss.item()
                _, predicted = outputs.max(1)
                total += labels.size(0)
                correct += predicted.eq(labels).sum().item()
        
        val_loss = running_loss / len(self.val_loader)
        val_acc = 100.0 * correct / total
        
        return val_loss, val_acc
    
    def run_experiment(self, config, exp_id):
        """Run single experiment with given configuration"""
        print(f"\n{'='*80}")
        print(f"üß™ EXPERIMENT {exp_id}")
        print(f"{'='*80}")
        print(f"üìä Config: LR={config['learning_rate']:.5f}, "
              f"WD={config['weight_decay']:.5f}, "
              f"Dropout={config['dropout']:.1f}")
        print(f"‚è±Ô∏è  Quick training: {self.quick_epochs} epochs")
        print(f"{'='*80}\n")
        
        start_time = time.time()
        
        # Create model with config
        model, optimizer, scheduler, criterion = self.create_model_with_config(config)
        scaler = GradScaler(enabled=torch.cuda.is_available())
        
        # Training history
        history = {
            'train_loss': [],
            'train_acc': [],
            'val_loss': [],
            'val_acc': []
        }
        
        best_val_acc = 0.0
        
        # Training loop
        for epoch in range(self.quick_epochs):
            # Train
            train_loss, train_acc = self.train_epoch(model, optimizer, criterion, scaler)
            
            # Validate
            val_loss, val_acc = self.validate(model, criterion)
            
            # Update scheduler
            scheduler.step()
            
            # Store history
            history['train_loss'].append(train_loss)
            history['train_acc'].append(train_acc)
            history['val_loss'].append(val_loss)
            history['val_acc'].append(val_acc)
            
            # Track best
            if val_acc > best_val_acc:
                best_val_acc = val_acc
            
            # Print progress
            print(f"Epoch {epoch+1}/{self.quick_epochs}: "
                  f"Train Loss={train_loss:.4f}, Train Acc={train_acc:.2f}% | "
                  f"Val Loss={val_loss:.4f}, Val Acc={val_acc:.2f}%")
        
        elapsed_time = time.time() - start_time
        
        # Save experiment results
        experiment_result = {
            'exp_id': exp_id,
            'config': config,
            'history': history,
            'best_val_acc': best_val_acc,
            'final_val_acc': history['val_acc'][-1],
            'elapsed_time': elapsed_time,
            'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        }
        
        # Save to file
        exp_file = self.experiments_dir / f'experiment_{exp_id}.json'
        with open(exp_file, 'w') as f:
            json.dump(experiment_result, f, indent=2)
        
        print(f"\n‚úÖ Experiment {exp_id} Complete!")
        print(f"   Best Val Acc: {best_val_acc:.2f}%")
        print(f"   Final Val Acc: {history['val_acc'][-1]:.2f}%")
        print(f"   Time: {elapsed_time/60:.1f} minutes")
        print(f"   Saved: {exp_file}")
        
        return experiment_result
    
    def run_sweep(self):
        """Run full hyperparameter sweep"""
        print("\n" + "="*80)
        print("üöÄ STARTING HYPERPARAMETER SWEEP")
        print("="*80 + "\n")
        
        # Generate configs
        configs = self.generate_configs()
        
        # Load data
        self.load_data()
        
        # Run experiments
        for i, config in enumerate(configs, 1):
            exp_id = f"exp_{i:03d}"
            result = self.run_experiment(config, exp_id)
            self.experiments.append(result)
            
            # Track best
            if result['best_val_acc'] > self.best_val_acc:
                self.best_val_acc = result['best_val_acc']
                self.best_config = config
                self.best_exp_id = exp_id
        
        # Save sweep summary
        self.save_sweep_summary()
    
    def save_sweep_summary(self):
        """Save summary of all experiments"""
        summary = {
            'total_experiments': len(self.experiments),
            'quick_epochs': self.quick_epochs,
            'search_space': self.search_space,
            'best_config': self.best_config,
            'best_val_acc': self.best_val_acc,
            'best_exp_id': self.best_exp_id,
            'all_experiments': [
                {
                    'exp_id': exp['exp_id'],
                    'config': exp['config'],
                    'best_val_acc': exp['best_val_acc'],
                    'final_val_acc': exp['final_val_acc']
                }
                for exp in self.experiments
            ],
            'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        }
        
        summary_file = self.experiments_dir / 'sweep_summary.json'
        with open(summary_file, 'w') as f:
            json.dump(summary, f, indent=2)
        
        print("\n" + "="*80)
        print("üìä SWEEP SUMMARY")
        print("="*80)
        print(f"\n‚úÖ Completed {len(self.experiments)} experiments")
        print(f"\nüèÜ Best Configuration:")
        print(f"   Experiment: {self.best_exp_id}")
        print(f"   Learning Rate: {self.best_config['learning_rate']:.5f}")
        print(f"   Weight Decay: {self.best_config['weight_decay']:.5f}")
        print(f"   Dropout: {self.best_config['dropout']:.1f}")
        print(f"   Best Val Acc: {self.best_val_acc:.2f}%")
        
        print(f"\nüìã All Experiments:")
        sorted_exps = sorted(self.experiments, key=lambda x: x['best_val_acc'], reverse=True)
        for i, exp in enumerate(sorted_exps, 1):
            config = exp['config']
            print(f"   {i}. {exp['exp_id']}: {exp['best_val_acc']:.2f}% "
                  f"(LR={config['learning_rate']:.5f}, "
                  f"WD={config['weight_decay']:.5f}, "
                  f"Dropout={config['dropout']:.1f})")
        
        print(f"\nüíæ Summary saved: {summary_file}")
        print("="*80 + "\n")
    
    def retrain_with_best_config(self):
        """Retrain model with best configuration for full epochs"""
        print("\n" + "="*80)
        print("üéØ FINAL RETRAIN WITH BEST CONFIG")
        print("="*80)
        print(f"üìä Best Config: LR={self.best_config['learning_rate']:.5f}, "
              f"WD={self.best_config['weight_decay']:.5f}, "
              f"Dropout={self.best_config['dropout']:.1f}")
        print(f"‚è±Ô∏è  Training: {self.final_epochs} epochs")
        print(f"üíæ Output: models/cropshield_cnn_best.pth")
        print("="*80 + "\n")
        
        start_time = time.time()
        
        # Create model
        model, optimizer, scheduler, criterion = self.create_model_with_config(self.best_config)
        scaler = GradScaler(enabled=torch.cuda.is_available())
        
        # Training history
        history = {
            'train_loss': [],
            'train_acc': [],
            'val_loss': [],
            'val_acc': []
        }
        
        best_val_acc = 0.0
        best_epoch = 0
        patience_counter = 0
        early_stopping_patience = 10
        
        # Training loop
        for epoch in range(self.final_epochs):
            print(f"\nEpoch {epoch+1}/{self.final_epochs}")
            
            # Train
            train_loss, train_acc = self.train_epoch(model, optimizer, criterion, scaler)
            
            # Validate
            val_loss, val_acc = self.validate(model, criterion)
            
            # Update scheduler
            scheduler.step()
            current_lr = optimizer.param_groups[0]['lr']
            
            # Store history
            history['train_loss'].append(train_loss)
            history['train_acc'].append(train_acc)
            history['val_loss'].append(val_loss)
            history['val_acc'].append(val_acc)
            
            # Print progress
            print(f"   Train: Loss={train_loss:.4f}, Acc={train_acc:.2f}%")
            print(f"   Val:   Loss={val_loss:.4f}, Acc={val_acc:.2f}%")
            print(f"   LR:    {current_lr:.6f}")
            
            # Check for best model
            if val_acc > best_val_acc:
                best_val_acc = val_acc
                best_epoch = epoch
                patience_counter = 0
                
                # Save best model
                checkpoint = {
                    'epoch': epoch,
                    'model_state_dict': model.state_dict(),
                    'optimizer_state_dict': optimizer.state_dict(),
                    'scheduler_state_dict': scheduler.state_dict(),
                    'best_val_acc': best_val_acc,
                    'history': history,
                    'num_classes': self.num_classes,
                    'class_names': self.class_names,
                    'model_type': 'custom',
                    'best_config': self.best_config,
                    'sweep_id': self.best_exp_id
                }
                
                model_path = self.models_dir / 'cropshield_cnn_best.pth'
                torch.save(checkpoint, model_path)
                print(f"   üéâ New best! Saved checkpoint")
            else:
                patience_counter += 1
                print(f"   No improvement ({patience_counter}/{early_stopping_patience})")
            
            # Early stopping
            if patience_counter >= early_stopping_patience:
                print(f"\n‚èπÔ∏è  Early stopping after {epoch+1} epochs")
                break
        
        elapsed_time = time.time() - start_time
        
        print("\n" + "="*80)
        print("‚úÖ FINAL RETRAIN COMPLETE!")
        print("="*80)
        print(f"‚è±Ô∏è  Training Time: {elapsed_time/60:.1f} minutes")
        print(f"üéØ Best Val Accuracy: {best_val_acc:.2f}% (Epoch {best_epoch+1})")
        print(f"üíæ Model Saved: {model_path}")
        print("="*80 + "\n")
        
        # Save training results
        final_results = {
            'best_config': self.best_config,
            'best_val_acc': best_val_acc,
            'best_epoch': best_epoch + 1,
            'total_epochs': epoch + 1,
            'training_time': elapsed_time,
            'history': history,
            'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        }
        
        results_file = self.experiments_dir / 'final_retrain_results.json'
        with open(results_file, 'w') as f:
            json.dump(final_results, f, indent=2)
        
        print(f"üìä Training results saved: {results_file}\n")
        
        return model_path, best_val_acc
    
    def run_full_pipeline(self):
        """Execute complete hyperparameter optimization pipeline"""
        try:
            # Step 1: Run hyperparameter sweep
            self.run_sweep()
            
            # Step 2: Retrain with best config
            model_path, best_val_acc = self.retrain_with_best_config()
            
            # Final summary
            print("\n" + "="*80)
            print("üéâ HYPERPARAMETER OPTIMIZATION COMPLETE!")
            print("="*80)
            print(f"\nüìä Results:")
            print(f"   Experiments Run: {len(self.experiments)}")
            print(f"   Best Config: LR={self.best_config['learning_rate']:.5f}, "
                  f"WD={self.best_config['weight_decay']:.5f}")
            print(f"   Best Val Acc: {best_val_acc:.2f}%")
            print(f"\nüìÅ Output Files:")
            print(f"   Model: {model_path}")
            print(f"   Sweep Summary: {self.experiments_dir / 'sweep_summary.json'}")
            print(f"   Final Results: {self.experiments_dir / 'final_retrain_results.json'}")
            print(f"   Experiments: {self.experiments_dir}/experiment_*.json")
            print("\n" + "="*80 + "\n")
            
            return True
            
        except Exception as e:
            print(f"\n‚ùå Error during optimization: {e}")
            import traceback
            traceback.print_exc()
            return False


def main():
    """Main entry point"""
    print("\n" + "="*80)
    print("üî¨ CROPSHIELD AI - AUTOMATED HYPERPARAMETER OPTIMIZATION")
    print("="*80)
    print(f"üìÖ Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("="*80 + "\n")
    
    # Create sweep
    sweep = HyperparameterSweep(
        quick_epochs=5,        # Quick experiments
        final_epochs=25,       # Final retrain
        batch_size=32,
        experiments_dir='experiments',
        models_dir='models'
    )
    
    # Run full pipeline
    success = sweep.run_full_pipeline()
    
    return success


if __name__ == '__main__':
    success = main()
    exit(0 if success else 1)
